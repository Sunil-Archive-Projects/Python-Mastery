{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equivalent-mouth",
   "metadata": {},
   "source": [
    "# Spark Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nonprofit-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developing-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('sparkBasics').master(\"local[*]\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sharing-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "listed-booking",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hourly-excuse",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dental-wheat",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "african-rider",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+------------------+-------+\n|summary|               age|   name|\n+-------+------------------+-------+\n|  count|                 2|      3|\n|   mean|              24.5|   null|\n| stddev|7.7781745930520225|   null|\n|    min|                19|   Andy|\n|    max|                30|Michael|\n+-------+------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-syndrome",
   "metadata": {},
   "source": [
    "# Defining Schema - For Strict Datatype Conformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blank-longitude",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- age: integer (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField,IntegerType,StringType,StructType\n",
    "\n",
    "data_schema = [StructField('age', IntegerType(), True),\n",
    "              StructField('name', StringType(), True)]\n",
    "\n",
    "final_struct = StructType(fields=data_schema)\n",
    "\n",
    "df = spark.read.json('people.json', schema=final_struct)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interim-cambridge",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n| age|\n+----+\n|null|\n|  30|\n|  19|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Select a particular Column from the DF\n",
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "robust-field",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----+\n|   name| age|\n+-------+----+\n|Michael|null|\n|   Andy|  30|\n| Justin|  19|\n+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Select Multiple Columns in a DF\n",
    "df.select(['name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "active-sweden",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Peek-a-boo 2 rows\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "subject-investing",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+------+\n| age|   name|newAge|\n+----+-------+------+\n|null|Michael|  null|\n|  30|   Andy|    60|\n|  19| Justin|    38|\n+----+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Add a new column to the DF\n",
    "df.withColumn('newAge',df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "imperial-circus",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+\n|new_age|   name|\n+-------+-------+\n|   null|Michael|\n|     30|   Andy|\n|     19| Justin|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Show a column with its alias name\n",
    "df.withColumnRenamed('age','new_age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "controversial-sector",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('people')\n",
    "\n",
    "results = spark.sql('SELECT * from people')\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-uncle",
   "metadata": {},
   "source": [
    "# Working with CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surface-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hourly-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName('ops').getOrCreate()\n",
    "# To resolve the binding address issue when running locally\n",
    "spark = SparkSession.builder.appName('ops').master(\"local[*]\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "relative-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('appl_stock.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "established-audit",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(Date='2010-01-04', Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "conservative-timothy",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- Date: string (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Volume: integer (nullable = true)\n |-- Adj Close: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "expanded-server",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+---------+\n|             Close|   Volume|\n+------------------+---------+\n|        214.009998|123432400|\n|        214.379993|150476200|\n|        210.969995|138040000|\n|            210.58|119282800|\n|211.98000499999998|111902700|\n|210.11000299999998|115557400|\n|        207.720001|148614900|\n|        210.650002|151473000|\n|            209.43|108223500|\n|            205.93|148516900|\n|        215.039995|182501900|\n|            211.73|153038200|\n|        208.069996|152038600|\n|            197.75|220441900|\n|        203.070002|266424900|\n|        205.940001|466777500|\n|        207.880005|430642100|\n|        199.289995|293375600|\n|        192.060003|311488100|\n|        194.729998|187469100|\n+------------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# select all the Volume and Close where Close < 500\n",
    "df.filter(df['Close'] < 500).select(['Close','Volume']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "laden-huntington",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n|2010-02-16|        201.940002|        203.690002|        201.520006|        203.399996|135934400|         26.352412|\n|2010-02-17|        204.190001|        204.310003|        200.860004|        202.550003|109099200|         26.242287|\n|2010-02-18|        201.629995|        203.889994|        200.920006|        202.929998|105706300|         26.291519|\n|2010-02-19|        201.860001|        203.200005|        201.109997|        201.669996|103867400|26.128273999999998|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# select with multiple conditions\n",
    "df.filter((df['Close'] > 200) & ~(df['Open'] < 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "civic-bridal",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Date': '2010-01-22',\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "#get the results of the Row objects as Dictionary\n",
    "result = df.filter(df['low']==197.16).collect()\n",
    "result[0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-northwest",
   "metadata": {},
   "source": [
    "# Groupby and Aggregate Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "light-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sales_info.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "vulnerable-connecticut",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- Company: string (nullable = true)\n |-- Person: string (nullable = true)\n |-- Sales: double (nullable = true)\n\n+-------+-------+-----+\n|Company| Person|Sales|\n+-------+-------+-----+\n|   GOOG|    Sam|200.0|\n|   GOOG|Charlie|120.0|\n|   GOOG|  Frank|340.0|\n|   MSFT|   Tina|600.0|\n|   MSFT|    Amy|124.0|\n|   MSFT|Vanessa|243.0|\n|     FB|   Carl|870.0|\n|     FB|  Sarah|350.0|\n|   APPL|   John|250.0|\n|   APPL|  Linda|130.0|\n|   APPL|   Mike|750.0|\n|   APPL|  Chris|350.0|\n+-------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----------------+\n|Company|       avg(Sales)|\n+-------+-----------------+\n|   APPL|            370.0|\n|   GOOG|            220.0|\n|     FB|            610.0|\n|   MSFT|322.3333333333333|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Company\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----+\n|Company|count|\n+-------+-----+\n|   APPL|    4|\n|   GOOG|    3|\n|     FB|    2|\n|   MSFT|    3|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Company\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n|sum(Sales)|\n+----------+\n|    4327.0|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+----------+\n",
      "|max(Sales)|\n",
      "+----------+\n",
      "|     870.0|\n",
      "+----------+\n",
      "\n",
      "+------------+\n",
      "|count(Sales)|\n",
      "+------------+\n",
      "|          12|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.agg({'Sales':'max'}).show()\n",
    "df.agg({'Sales':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+------------+\n",
      "|Company|count(Sales)|\n",
      "+-------+------------+\n",
      "|   APPL|           4|\n",
      "|   GOOG|           3|\n",
      "|     FB|           2|\n",
      "|   MSFT|           3|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_data = df.groupBy(\"Company\")\n",
    "grouped_data.agg({'Sales':'max'}).show()\n",
    "grouped_data.agg({'Sales':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+-----+\n|Company| Person|Sales|\n+-------+-------+-----+\n|   GOOG|    Sam|200.0|\n|   GOOG|Charlie|120.0|\n|   GOOG|  Frank|340.0|\n|   MSFT|   Tina|600.0|\n|   MSFT|    Amy|124.0|\n|   MSFT|Vanessa|243.0|\n|     FB|   Carl|870.0|\n|     FB|  Sarah|350.0|\n|   APPL|   John|250.0|\n|   APPL|  Linda|130.0|\n|   APPL|   Mike|750.0|\n|   APPL|  Chris|350.0|\n+-------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev, format_number\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+\n",
      "|Distinct Counts|\n",
      "+---------------+\n",
      "|             11|\n",
      "+---------------+\n",
      "\n",
      "+-----------------+\n",
      "|    Average Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n",
      "+------------------+\n",
      "|stddev_samp(Sales)|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Sales').alias('Distinct Counts')).show()\n",
    "df.select(avg('Sales').alias('Average Sales')).show()\n",
    "df.select(stddev('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}