{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('wordCount').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----+\n|    word|count|\n+--------+-----+\n|     you| 1878|\n|      to| 1828|\n|    your| 1420|\n|     the| 1292|\n|       a| 1191|\n|      of|  970|\n|     and|  934|\n|    that|  747|\n|      it|  649|\n|      in|  616|\n|      is|  560|\n|     for|  537|\n|      on|  428|\n|     are|  424|\n|      if|  411|\n|       s|  391|\n|       i|  387|\n|business|  383|\n|     can|  376|\n|      be|  369|\n+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, lower, count\n",
    "\n",
    "data = spark.read.text(\"book.txt\")\n",
    "data = data.select(explode(split(data.value, \"\\\\W+\")))\n",
    "data = data.filter(data['col'] != \"\")\n",
    "words = data.select(lower(data['col']).alias(\"word\")).groupBy(\"word\").agg(count(\"word\").alias(\"count\"))\n",
    "words = words.orderBy(words[\"count\"].desc())\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('word_count_test').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|     you| 1878|\n",
      "|      to| 1828|\n",
      "|    your| 1420|\n",
      "|     the| 1292|\n",
      "|       a| 1191|\n",
      "|      of|  970|\n",
      "|     and|  934|\n",
      "|        |  772|\n",
      "|    that|  747|\n",
      "|      it|  649|\n",
      "|      in|  616|\n",
      "|      is|  560|\n",
      "|     for|  537|\n",
      "|      on|  428|\n",
      "|     are|  424|\n",
      "|      if|  411|\n",
      "|       s|  391|\n",
      "|       i|  387|\n",
      "|business|  383|\n",
      "|     can|  376|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "4119\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, lower, count\n",
    "\n",
    "text_data = spark.read.text('book.txt')\n",
    "\n",
    "text_data = text_data.select(explode(split(text_data.value, \"\\\\W+\")))\n",
    "\n",
    "text_data = text_data.select(lower(text_data['col']).alias('word')).groupBy(\"word\").agg(count(\"word\").alias(\"count\"))\n",
    "\n",
    "text_data = text_data.orderBy(text_data[\"count\"].desc())\n",
    "\n",
    "text_data.show()\n",
    "\n",
    "print(text_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('test_books_wordcount').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------+\n|col     |counts|\n+--------+------+\n|to      |1816  |\n|you     |1690  |\n|your    |1340  |\n|the     |1178  |\n|a       |1149  |\n|of      |951   |\n|and     |910   |\n|        |772   |\n|that    |705   |\n|in      |574   |\n|it      |548   |\n|is      |542   |\n|for     |515   |\n|on      |426   |\n|are     |401   |\n|I       |387   |\n|s       |385   |\n|be      |358   |\n|can     |355   |\n|business|350   |\n+--------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, split, explode\n",
    "\n",
    "df = spark.read.text('book.txt')\n",
    "\n",
    "df = df.select(explode(split(df.value, \"\\\\W+\"))).alias('col')\n",
    "\n",
    "df = df.groupBy('col')\n",
    "df = df.agg(count(\"col\").alias(\"counts\"))\n",
    "\n",
    "df = df.orderBy(df[\"counts\"].desc())\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}