{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nLet’s write a Spark program that reads a file with over 100,000 entries (where each\\nrow or line has a <state, mnm_color, count>) and computes and aggregates the\\ncounts for each color and state. These aggregated counts tell us the colors of M&Ms\\nfavored by students in each state.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let’s write a Spark program that reads a file with over 100,000 entries (where each\n",
    "row or line has a <state, mnm_color, count>) and computes and aggregates the\n",
    "counts for each color and state. These aggregated counts tell us the colors of M&Ms\n",
    "favored by students in each state.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('mnm_count').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------+-----+\n|State|Color |Count|\n+-----+------+-----+\n|TX   |Red   |20   |\n|NV   |Blue  |66   |\n|CO   |Blue  |79   |\n|OR   |Blue  |71   |\n|WA   |Yellow|93   |\n|WY   |Blue  |16   |\n|CA   |Yellow|53   |\n|WA   |Green |60   |\n|OR   |Green |71   |\n|TX   |Green |68   |\n|NV   |Green |59   |\n|AZ   |Brown |95   |\n|WA   |Yellow|20   |\n|AZ   |Blue  |75   |\n|OR   |Brown |72   |\n|NV   |Red   |98   |\n|WY   |Orange|45   |\n|CO   |Blue  |52   |\n|TX   |Brown |94   |\n|CO   |Red   |82   |\n+-----+------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#read the csv data into dataframe\n",
    "raw_df = spark.read.csv(\"./data/mnm_dataset_for_count/mnm_dataset.csv\", inferSchema= True, header= True)\n",
    "\n",
    "#display the first 20 rows of the df\n",
    "raw_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------+----+----+----+----+----+----+----+----+----+----+\n|State| Color|  AZ|  CA|  CO|  NM|  NV|  OR|  TX|  UT|  WA|  WY|\n+-----+------+----+----+----+----+----+----+----+----+----+----+\n|   WY| Green|null|null|null|null|null|null|null|null|null|1695|\n|   NV|   Red|null|null|null|null|1610|null|null|null|null|null|\n|   UT|  Blue|null|null|null|null|null|null|null|1655|null|null|\n|   WA|Orange|null|null|null|null|null|null|null|null|1658|null|\n|   NM| Green|null|null|null|1682|null|null|null|null|null|null|\n|   CA|  Blue|null|1603|null|null|null|null|null|null|null|null|\n|   WA|   Red|null|null|null|null|null|null|null|null|1671|null|\n|   NV| Brown|null|null|null|null|1657|null|null|null|null|null|\n|   AZ| Green|1676|null|null|null|null|null|null|null|null|null|\n|   CA|   Red|null|1656|null|null|null|null|null|null|null|null|\n|   AZ|Orange|1689|null|null|null|null|null|null|null|null|null|\n|   CO|  Blue|null|null|1695|null|null|null|null|null|null|null|\n|   NM|Orange|null|null|null|1665|null|null|null|null|null|null|\n|   NM|Yellow|null|null|null|1688|null|null|null|null|null|null|\n|   WY|Orange|null|null|null|null|null|null|null|null|null|1595|\n|   UT|Yellow|null|null|null|null|null|null|null|1645|null|null|\n|   WY|   Red|null|null|null|null|null|null|null|null|null|1670|\n|   OR|  Blue|null|null|null|null|null|1646|null|null|null|null|\n|   NV|Orange|null|null|null|null|1712|null|null|null|null|null|\n|   AZ|Yellow|1654|null|null|null|null|null|null|null|null|null|\n+-----+------+----+----+----+----+----+----+----+----+----+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# raw_df grouped by Color\n",
    "grouped_df = raw_df.select(\"State\", \"Color\", \"Count\").groupBy(\"State\", \"Color\")\n",
    "\n",
    "#workaround for seeing the grouped data since show is not supported by groupedData directly\n",
    "grouped_df.pivot(\"State\").count().show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------+-----+\n|State| Color|Total|\n+-----+------+-----+\n|   CA|Yellow| 1807|\n|   WA| Green| 1779|\n|   OR|Orange| 1743|\n|   TX| Green| 1737|\n|   TX|   Red| 1725|\n|   CA| Green| 1723|\n|   CO|Yellow| 1721|\n|   CA| Brown| 1718|\n|   CO| Green| 1713|\n|   NV|Orange| 1712|\n|   TX|Yellow| 1703|\n|   NV| Green| 1698|\n|   AZ| Brown| 1698|\n|   CO|  Blue| 1695|\n|   WY| Green| 1695|\n|   NM|   Red| 1690|\n|   AZ|Orange| 1689|\n|   NM|Yellow| 1688|\n|   NM| Brown| 1687|\n|   UT|Orange| 1684|\n|   NM| Green| 1682|\n|   UT|   Red| 1680|\n|   AZ| Green| 1676|\n|   NV|Yellow| 1675|\n|   NV|  Blue| 1673|\n|   WA|   Red| 1671|\n|   WY|   Red| 1670|\n|   WA| Brown| 1669|\n|   NM|Orange| 1665|\n|   WY|  Blue| 1664|\n|   WA|Yellow| 1663|\n|   WA|Orange| 1658|\n|   CA|Orange| 1657|\n|   NV| Brown| 1657|\n|   CO| Brown| 1656|\n|   CA|   Red| 1656|\n|   UT|  Blue| 1655|\n|   AZ|Yellow| 1654|\n|   TX|Orange| 1652|\n|   AZ|   Red| 1648|\n|   OR|  Blue| 1646|\n|   OR|   Red| 1645|\n|   UT|Yellow| 1645|\n|   CO|Orange| 1642|\n|   TX| Brown| 1641|\n|   NM|  Blue| 1638|\n|   AZ|  Blue| 1636|\n|   OR| Green| 1634|\n|   UT| Brown| 1631|\n|   WY|Yellow| 1626|\n|   WA|  Blue| 1625|\n|   CO|   Red| 1624|\n|   OR| Brown| 1621|\n|   TX|  Blue| 1614|\n|   OR|Yellow| 1614|\n|   NV|   Red| 1610|\n|   CA|  Blue| 1603|\n|   WY|Orange| 1595|\n|   UT| Green| 1591|\n|   WY| Brown| 1532|\n+-----+------+-----+\n\nTotal Rows : 99999\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "grouped_df_with_count = grouped_df.agg(count(\"Count\").alias(\"Total\")).orderBy(\"Total\", ascending= False)\n",
    "grouped_df_with_count.show(100)\n",
    "\n",
    "print(\"Total Rows : \"+str(raw_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While the above code aggregated and counted for all\n",
    "# the states, what if we just want to see the data for\n",
    "# a single state, e.g., CA?\n",
    "# 1. Select from all rows in the DataFrame\n",
    "# 2. Filter only CA state\n",
    "# 3. groupBy() State and Color as we did above\n",
    "# 4. Aggregate the counts for each color\n",
    "# 5. orderBy() in descending order\n",
    "# Find the aggregate count for California by filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------+-----+\n|State|Color |Total|\n+-----+------+-----+\n|CA   |Yellow|1807 |\n|CA   |Green |1723 |\n|CA   |Brown |1718 |\n|CA   |Orange|1657 |\n|CA   |Red   |1656 |\n|CA   |Blue  |1603 |\n+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "ca_data = raw_df.select(\"State\", \"Color\", \"Count\").where(raw_df.State== \"CA\").groupBy(\"State\", \"Color\").agg(count(\"Count\").alias(\"Total\")).orderBy(\"Total\", ascending = False).show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}